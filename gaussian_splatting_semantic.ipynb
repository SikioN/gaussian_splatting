{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUytbAd92HaR"
   },
   "source": [
    "# Импорт библиотек  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем предпочтительную кодировку UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-25T11:29:13.126002121Z",
     "iopub.status.idle": "2025-06-25T11:29:13.135973804Z",
     "shell.execute_reply": "2025-06-25T11:29:13.125971602Z"
    },
    "id": "Q1UkX4Xj2BdU",
    "tags": []
   },
   "outputs": [
    {
     "ename": "Unknown instance spec",
     "evalue": "Please select VM configuration",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:22:12.526698Z",
     "iopub.status.busy": "2025-06-25T01:22:12.525437Z",
     "iopub.status.idle": "2025-06-25T01:22:12.551803Z",
     "shell.execute_reply": "2025-06-25T01:22:12.550751Z",
     "shell.execute_reply.started": "2025-06-25T01:22:12.526647Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/clip\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/datasphere/project/clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/camenduru/gaussian-splatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:08:34.868576Z",
     "iopub.status.busy": "2025-06-25T02:08:34.866894Z",
     "iopub.status.idle": "2025-06-25T02:08:40.835308Z",
     "shell.execute_reply": "2025-06-25T02:08:40.834084Z",
     "shell.execute_reply.started": "2025-06-25T02:08:34.868493Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plyfile in /home/jupyter/.local/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/jupyter/.local/lib/python3.10/site-packages (from plyfile) (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install plyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-25T12:30:22.998782762Z",
     "iopub.status.idle": "2025-06-25T12:30:22.999865019Z",
     "shell.execute_reply": "2025-06-25T12:30:22.998756900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Unknown instance spec",
     "evalue": "Please select VM configuration",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "%cd /home/jupyter/datasphere/project/clip/gaussian-splatting\n",
    "\n",
    "# Очистка старых сборок\n",
    "!rm -rf submodules/diff-gaussian-rasterization/build\n",
    "!rm -rf submodules/simple-knn/build\n",
    "\n",
    "# Установка с правильной архитектурой\n",
    "!TORCH_CUDA_ARCH_LIST=\"7.0\"\n",
    "%pip install submodules/simple-knn/\n",
    "!TORCH_CUDA_ARCH_LIST=\"7.0\"\n",
    "%pip install submodules/diff-gaussian-rasterization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T00:12:23.246757Z",
     "iopub.status.busy": "2025-06-25T00:12:23.245725Z",
     "iopub.status.idle": "2025-06-25T00:13:12.619672Z",
     "shell.execute_reply": "2025-06-25T00:13:12.618354Z",
     "shell.execute_reply.started": "2025-06-25T00:12:23.246708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -q vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-25T00:24:56.969764Z",
     "iopub.status.busy": "2025-06-25T00:24:56.968701Z",
     "iopub.status.idle": "2025-06-25T00:26:15.765488Z",
     "shell.execute_reply": "2025-06-25T00:26:15.764586Z",
     "shell.execute_reply.started": "2025-06-25T00:24:56.969710Z"
    },
    "id": "RUQ2tDHxBNwG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "defa3107-b7b9-472e-ffec-4131acb3a0e4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/facebookresearch/ImageBind\n",
      "  Cloning https://github.com/facebookresearch/ImageBind to /tmp/pip-req-build-6o9e8mio\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/ImageBind /tmp/pip-req-build-6o9e8mio\n",
      "  Resolved https://github.com/facebookresearch/ImageBind to commit 3fcf5c9039de97f6ff5528ee4a9dce903c5979b3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch==1.13.1 in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from imagebind==0.1.0) (2.0.2+cu118)\n",
      "Collecting pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d (from imagebind==0.1.0)\n",
      "  Cloning https://github.com/facebookresearch/pytorchvideo.git (to revision 28fe037d212663c6a24f373b94cc5d478c8c1a1d) to /tmp/pip-install-yt8_wfwq/pytorchvideo_e8910367c2d241a0a5ec7fb52236f94a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-install-yt8_wfwq/pytorchvideo_e8910367c2d241a0a5ec7fb52236f94a\n",
      "  Running command git rev-parse -q --verify 'sha^28fe037d212663c6a24f373b94cc5d478c8c1a1d'\n",
      "  Running command git fetch -q https://github.com/facebookresearch/pytorchvideo.git 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
      "  Running command git checkout -q 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
      "  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting timm==0.6.7 (from imagebind==0.1.0)\n",
      "  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: ftfy in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (6.3.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from imagebind==0.1.0) (2022.10.31)\n",
      "Requirement already satisfied: einops in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: fvcore in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (0.1.5.post20221221)\n",
      "Requirement already satisfied: eva-decord==0.6.1 in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: iopath in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (2.2.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imagebind==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: types-regex in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (2024.11.6.20250403)\n",
      "Collecting mayavi (from imagebind==0.1.0)\n",
      "  Downloading mayavi-4.8.3.tar.gz (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cartopy in /home/jupyter/.local/lib/python3.10/site-packages (from imagebind==0.1.0) (0.24.1)\n",
      "Requirement already satisfied: av in /home/jupyter/.local/lib/python3.10/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (14.4.0)\n",
      "Requirement already satisfied: parameterized in /home/jupyter/.local/lib/python3.10/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (3.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->imagebind==0.1.0) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==1.13.1->imagebind==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==1.13.1->imagebind==0.1.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==1.13.1->imagebind==0.1.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch==1.13.1->imagebind==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->imagebind==0.1.0) (65.5.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->imagebind==0.1.0) (0.41.1)\n",
      "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.10/dist-packages (from cartopy->imagebind==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: packaging>=21 in /home/jupyter/.local/lib/python3.10/site-packages (from cartopy->imagebind==0.1.0) (25.0)\n",
      "Requirement already satisfied: pyshp>=2.3 in /home/jupyter/.local/lib/python3.10/site-packages (from cartopy->imagebind==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from cartopy->imagebind==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagebind==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyproj>=3.3.1->cartopy->imagebind==0.1.0) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->imagebind==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->imagebind==0.1.0) (0.2.6)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/jupyter/.local/lib/python3.10/site-packages (from fvcore->imagebind==0.1.0) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->imagebind==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->imagebind==0.1.0) (4.65.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->imagebind==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->imagebind==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /home/jupyter/.local/lib/python3.10/site-packages (from iopath->imagebind==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: apptools in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (5.3.1)\n",
      "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from mayavi->imagebind==0.1.0) (5.0.8)\n",
      "Requirement already satisfied: envisage in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (7.0.4)\n",
      "Requirement already satisfied: pyface>=6.1.1 in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from mayavi->imagebind==0.1.0) (2.14.0)\n",
      "Requirement already satisfied: traits>=6.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (7.0.2)\n",
      "Requirement already satisfied: traitsui>=7.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: vtk in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (9.5.0)\n",
      "Requirement already satisfied: puremagic in /home/jupyter/.local/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (1.29)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from mayavi->imagebind==0.1.0) (6.0.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio (from imagebind==0.1.0)\n",
      "  Downloading torchaudio-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchaudio-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.7 kB)\n",
      "  Downloading torchaudio-2.0.2-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "  Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->imagebind==0.1.0) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->imagebind==0.1.0) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->imagebind==0.1.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->imagebind==0.1.0) (3.4)\n",
      "Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
      "Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: imagebind, mayavi\n",
      "\u001b[33m  DEPRECATION: Building 'imagebind' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'imagebind'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for imagebind (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imagebind: filename=imagebind-0.1.0-py3-none-any.whl size=1385511 sha256=d671072ba6188a5e011109de6b17074761768f141c48caf06947107474ac6149\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fvf5uemb/wheels/31/ec/41/0ef4b0b43b409d273367290a4fc45040eb25d50e8345438148\n",
      "  Building wheel for mayavi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mayavi: filename=mayavi-4.8.3-cp310-cp310-linux_x86_64.whl size=17507841 sha256=b08dddbfb86905e2f3ac9b787dd55c8bfd3dba6412b64578210f3fecfaa293af\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/d4/62/e4/d7f2b6365df0220405ab7cc26a47effc9f6266138769c008ae\n",
      "Successfully built imagebind mayavi\n",
      "Installing collected packages: torchaudio, mayavi, timm, imagebind\n",
      "\u001b[2K  Attempting uninstall: timm\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [mayavi]\n",
      "\u001b[2K    Found existing installation: timm 1.0.15━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [mayavi]\n",
      "\u001b[2K    Uninstalling timm-1.0.15:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [timm]\n",
      "\u001b[2K      Successfully uninstalled timm-1.0.15\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [timm]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [imagebind]/4\u001b[0m [imagebind]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imagebind-0.1.0 mayavi-4.8.3 timm-0.6.7 torchaudio-0.13.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/facebookresearch/ImageBind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T00:05:56.534886Z",
     "iopub.status.busy": "2025-06-25T00:05:56.533332Z",
     "iopub.status.idle": "2025-06-25T00:06:25.020730Z",
     "shell.execute_reply": "2025-06-25T00:06:25.019678Z",
     "shell.execute_reply.started": "2025-06-25T00:05:56.534829Z"
    },
    "id": "maK_YB6AwFjM",
    "outputId": "dfd8c31e-310a-4781-e5e7-127278f35129",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script ftfy is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts huggingface-cli and tiny-agents are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -q open_clip_torch celluloid matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:09:03.836006Z",
     "iopub.status.busy": "2025-06-25T02:09:03.834419Z",
     "iopub.status.idle": "2025-06-25T02:09:13.192332Z",
     "shell.execute_reply": "2025-06-25T02:09:13.190801Z",
     "shell.execute_reply.started": "2025-06-25T02:09:03.835969Z"
    },
    "id": "85O9kaUBwYU1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/clip/gaussian-splatting\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/datasphere/project/clip/gaussian-splatting\n",
    "\n",
    "# Импорт стандартных и внешних библиотек\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from random import randint\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # для отображения прогресса\n",
    "\n",
    "# Импорт из PyTorch\n",
    "import torch\n",
    "\n",
    "# Импорт утилит проекта\n",
    "from utils.loss_utils import l1_loss, ssim  # функции для вычисления потерь\n",
    "from utils.sh_utils import SH2RGB           # преобразование сферических гармоник в RGB\n",
    "from utils.general_utils import safe_state  # сохранение состояния модели\n",
    "from utils.image_utils import psnr          # вычисление PSNR (качество изображения)\n",
    "\n",
    "# Импорт рендерера и интерфейса\n",
    "from gaussian_renderer import render, network_gui\n",
    "\n",
    "# Импорт компонентов сцены\n",
    "from scene import Scene, GaussianModel\n",
    "from scene.cameras import Camera\n",
    "from scene.gaussian_model import BasicPointCloud\n",
    "\n",
    "# Импорт параметров модели, оптимизации и пайплайна\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams\n",
    "\n",
    "# Импорт библиотеки для работы с изображениями\n",
    "from PIL import Image\n",
    "\n",
    "# Импорт OpenCLIP — модели для сопоставления текста и изображений\n",
    "import open_clip\n",
    "\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка предобученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель CLIP используется для оценки соответствия между текстовым описанием и сгенерированным изображением, выступая в роли дифференцируемого критика (loss-функции) в процессе оптимизации параметров гауссиан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a47289814d1044419e4912e6b3e19c42",
      "cf626f8d3fd048b4a7b74c69d8fab2d4",
      "bb898f4ccc2f4958b8c086d2ac3142a5",
      "db1fb2dddc38406cb92a8a22ac410828",
      "f008a1a83a524a04b15ba1468e9911d6",
      "e0812836e6734413b4679cfe0a8c6844",
      "b8c5a59ed4f1441c95e46bd2b70aad18",
      "4b67fe9364194dde98fba540ae3d3391",
      "d594d6782180447881fa68b8e39f13e2",
      "ceb88275c4044807a17ecafca5ce8d72",
      "faead77034ea40a99f7344a7e2b50934"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T02:09:14.971584Z",
     "iopub.status.busy": "2025-06-25T02:09:14.970653Z",
     "iopub.status.idle": "2025-06-25T02:09:41.832441Z",
     "shell.execute_reply": "2025-06-25T02:09:41.831117Z",
     "shell.execute_reply.started": "2025-06-25T02:09:14.971534Z"
    },
    "id": "0rwp6QM_wePx",
    "outputId": "b0e332a1-5412-4486-a6db-4bbae3ad3393",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "oclip = 'ViT-B-32'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(oclip, pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "model.requires_grad_(False).cuda().half()\n",
    "tokenizer = open_clip.get_tokenizer(oclip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед подачей изображений в модель CLIP необходимо привести их к ожидаемому формату с помощью нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T02:09:41.835223Z",
     "iopub.status.busy": "2025-06-25T02:09:41.834201Z",
     "iopub.status.idle": "2025-06-25T02:09:41.853486Z",
     "shell.execute_reply": "2025-06-25T02:09:41.852507Z",
     "shell.execute_reply.started": "2025-06-25T02:09:41.835174Z"
    },
    "id": "SJT7jLCjwgnX",
    "outputId": "9a6e806b-2f3c-4408-bfff-80c1e0f70648",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clippp = Normalize(mean=model.visual.image_mean, std=model.visual.image_std)\n",
    "clippp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:09:41.855110Z",
     "iopub.status.busy": "2025-06-25T02:09:41.854503Z",
     "iopub.status.idle": "2025-06-25T02:09:42.982623Z",
     "shell.execute_reply": "2025-06-25T02:09:42.981555Z",
     "shell.execute_reply.started": "2025-06-25T02:09:41.855060Z"
    },
    "id": "1FN4zCLDwi2Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом фрагменте создаётся синтетическое облако точек — полезное для тестирования и визуализации в сценах, не требующих реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T02:09:42.986625Z",
     "iopub.status.busy": "2025-06-25T02:09:42.985462Z",
     "iopub.status.idle": "2025-06-25T02:09:43.021870Z",
     "shell.execute_reply": "2025-06-25T02:09:43.020714Z",
     "shell.execute_reply.started": "2025-06-25T02:09:42.986557Z"
    },
    "id": "RC_7vPhQwkpk",
    "outputId": "44e17e65-6963-493c-82b8-2a752f80c6cb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random point cloud (100)...\n"
     ]
    }
   ],
   "source": [
    "num_pts = 100\n",
    "print(f\"Generating random point cloud ({num_pts})...\")\n",
    "\n",
    "# Пространственные границы, в пределах которых будут размещены точки\n",
    "camera_extent = 5.0\n",
    "\n",
    "# Генерация случайных координат (x, y, z) с нормальным распределением\n",
    "xyz = np.random.normal(size=(num_pts, 3))\n",
    "\n",
    "# Масштабирование и нормализация — точки размещаются на сфере радиуса ≈1.3\n",
    "xyz = 1.3 * xyz / np.linalg.norm(xyz, axis=-1, keepdims=True)\n",
    "\n",
    "# Случайные цвета для точек, заданные в виде сферических гармоник (SH)\n",
    "shs = np.random.random((num_pts, 3)) / 255.0\n",
    "\n",
    "# Создание объекта облака точек без нормалей\n",
    "pcd = BasicPointCloud(\n",
    "    points=xyz,                # координаты точек\n",
    "    colors=SH2RGB(shs),        # преобразование SH → RGB\n",
    "    normals=np.zeros((num_pts, 3))  # нормали пока не используются\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция создаёт несколько случайных кропов с масштабированием и трансформацией, полезных для обучения моделей с аугментацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:09:43.024969Z",
     "iopub.status.busy": "2025-06-25T02:09:43.023123Z",
     "iopub.status.idle": "2025-06-25T02:09:43.042311Z",
     "shell.execute_reply": "2025-06-25T02:09:43.041359Z",
     "shell.execute_reply.started": "2025-06-25T02:09:43.024916Z"
    },
    "id": "JZ0a2jetwmYz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_crops(img, n, resize_to=224, min_size=0.5, max_size=1.0):\n",
    "    sizes = torch.rand(n) * (max_size - min_size) + min_size\n",
    "    top_left = torch.rand(n, 2) * (1.0 - sizes[:, None])\n",
    "    transforms = torch.eye(2).unsqueeze(0).repeat(n, 1, 1) * sizes[:, None, None]\n",
    "    transforms = torch.cat([transforms, top_left[:, :, None]], dim=-1)\n",
    "    grids = torch.nn.functional.affine_grid(transforms, [n, img.shape[1], resize_to, resize_to], align_corners=True).to(img.device)\n",
    "    return torch.nn.functional.grid_sample(img.repeat(n, 1, 1, 1), grids, align_corners=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот блок отвечает за генерацию случайного ракурса камеры, рендеринг сцены с текущими параметрами гауссиан, а затем сравнение полученного изображения с текстовым описанием с помощью модели CLIP.\n",
    "\n",
    "Основные шаги:\n",
    "\n",
    "- Генерируется случайная позиция камеры вокруг объекта и формируется матрица поворота и сдвига (`look_at`).\n",
    "- Создается объект камеры с заданными параметрами обзора и разрешением.\n",
    "- Выполняется рендеринг изображения сцены из заданного ракурса.\n",
    "- Текстовый запрос форматируется с учетом выбранного ракурса.\n",
    "- С помощью CLIP происходит кодирование изображения (с применением случайных обрезок) и текста в векторные представления.\n",
    "- Вычисляется косинусное сходство между этими векторами, формируя функцию потерь.\n",
    "- Потеря используется для обратного распространения ошибки и оптимизации параметров модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:10:25.788125Z",
     "iopub.status.busy": "2025-06-25T02:10:25.786794Z",
     "iopub.status.idle": "2025-06-25T02:10:47.971458Z",
     "shell.execute_reply": "2025-06-25T02:10:47.970228Z",
     "shell.execute_reply.started": "2025-06-25T02:10:25.788071Z"
    },
    "id": "kG3-fL7Vwotg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 02:10:30.048335: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-25 02:10:36.453887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from celluloid import Camera as PltCamera\n",
    "from math import floor\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TENSORBOARD_FOUND = True\n",
    "except ImportError:\n",
    "    TENSORBOARD_FOUND = False\n",
    "\n",
    "def rot2d(angle):\n",
    "    return torch.tensor([\n",
    "        [torch.cos(angle), torch.sin(angle), torch.tensor(0)],\n",
    "       [-torch.sin(angle), torch.cos(angle), torch.tensor(0)],\n",
    "        [torch.tensor(0), torch.tensor(0), torch.tensor(1)]\n",
    "    ])\n",
    "\n",
    "def normalize(x):\n",
    "    return x / x.norm(dim=-1, keepdim=True)\n",
    "def look_at(eye, at, up):\n",
    "    z = normalize(eye - at)\n",
    "    x = normalize(torch.linalg.cross(up, z))\n",
    "    y = normalize(torch.linalg.cross(z, x))\n",
    "    rot = torch.stack([x, y, z]).T\n",
    "    trans = torch.tensor([x @ eye, y @ eye, z @ eye])\n",
    "    return rot, trans\n",
    "\n",
    "\n",
    "def show(tensor):\n",
    "    tensor = tensor.detach().permute(1, 2, 0)\n",
    "    plt.imshow(tensor.cpu().numpy())\n",
    "\n",
    "def training(prompt, dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from):\n",
    "    first_iter = 0\n",
    "    gaussians = GaussianModel(dataset.sh_degree)\n",
    "    # scene = Scene(dataset, gaussians)\n",
    "    print(gaussians)\n",
    "    gaussians.create_from_pcd(pcd, camera_extent)\n",
    "    gaussians.training_setup(opt)\n",
    "    if checkpoint:\n",
    "        (model_params, first_iter) = torch.load(checkpoint)\n",
    "        gaussians.restore(model_params, opt)\n",
    "\n",
    "    bg_color = [0, 0, 0]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    iter_start = torch.cuda.Event(enable_timing = True)\n",
    "    iter_end = torch.cuda.Event(enable_timing = True)\n",
    "\n",
    "    viewpoint_stack = None\n",
    "    ema_loss_for_log = 0.0\n",
    "    progress_bar = tqdm(range(first_iter, opt.iterations), desc=\"Training progress\")\n",
    "    first_iter += 1\n",
    "    # dbgfig = plt.figure()\n",
    "    fig = plt.figure()\n",
    "    camera = PltCamera(fig)\n",
    "\n",
    "    for iteration in range(first_iter, opt.iterations + 1):\n",
    "\n",
    "        iter_start.record()\n",
    "\n",
    "        gaussians.update_learning_rate(iteration)\n",
    "\n",
    "        # Every 1000 its we increase the levels of SH up to a maximum degree\n",
    "        if iteration % 1000 == 0:\n",
    "            gaussians.oneupSHdegree()\n",
    "\n",
    "        # Pick a random Camera\n",
    "        # if not viewpoint_stack:\n",
    "        #    viewpoint_stack = scene.getTrainCameras().copy()\n",
    "        # viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))\n",
    "        rand_angle = torch.rand(1) * 3.14159 * 2.0\n",
    "        rand_pos = camera_extent * torch.tensor([rand_angle.cos(), 0.0, rand_angle.sin()])\n",
    "        rand_pos[1] = 0.2 + (torch.rand(1) * 2 - 1) * 0.3\n",
    "        dist, direc = torch.linalg.norm(rand_pos), normalize(rand_pos)\n",
    "        rand_pos = direc * camera_extent\n",
    "\n",
    "\n",
    "        # if (iteration % 10) == 0:\n",
    "        #     rand_pos = torch.tensor([0.0, 0.0, camera_extent])\n",
    "        #     rand_pos = rand_pos + torch.tensor([torch.tensor(3.14159 * 4.0 * iteration / opt.iterations).cos(), torch.tensor(3.14159 * 4.0 * iteration / opt.iterations).sin(), 0.0])\n",
    "\n",
    "        rot, trans = look_at(rand_pos, torch.tensor([0.0, 0.0, 0.0]), torch.tensor([0.0, 1.0, 0.0]))\n",
    "\n",
    "        train_res = floor(512 * (iteration / opt.iterations))\n",
    "        train_res = 32 * (1 + train_res // 32)\n",
    "        viewpoint_cam = Camera(\n",
    "            colmap_id=0,\n",
    "            R=rot.numpy(), #rand_angle.numpy(),\n",
    "            T=trans.numpy(), #torch.zeros((3,)).numpy(),\n",
    "            FoVx=90.0,\n",
    "            FoVy=90.0,\n",
    "            image=torch.zeros((3, train_res, train_res)),\n",
    "            gt_alpha_mask=torch.ones((train_res, train_res)),\n",
    "            image_name=\"test\",\n",
    "            uid=0,\n",
    "        )\n",
    "        # Render\n",
    "        if (iteration - 1) == debug_from:\n",
    "            pipe.debug = True\n",
    "        render_pkg = render(viewpoint_cam, gaussians, pipe, bg_color=torch.rand(3).cuda())\n",
    "        image, viewspace_point_tensor, visibility_filter, radii = render_pkg[\"render\"], render_pkg[\"viewspace_points\"], render_pkg[\"visibility_filter\"], render_pkg[\"radii\"]\n",
    "\n",
    "        # Loss\n",
    "\n",
    "        views = [\"front view\", \"left side view\", \"back view\", \"right side view\"]\n",
    "        qangle = floor(4 * rand_angle / (2 * 3.14159))\n",
    "        view = views[qangle]\n",
    "\n",
    "        prompt = prompt.format(view)\n",
    "        #text = tokenizer([\"a photo of a beautiful twink statue, not noisy, not rainbow, cute, accurate anatomy, black background, greeco-roman statue\"]).cuda()\n",
    "        #text = tokenizer([f\"a photo of a beautiful twink statue, {view}, not noisy, not rainbow, cute, accurate anatomy, black background, greeco-roman statue\"]).cuda()\n",
    "        #text = tokenizer([\"a 3d model of a cyber-sigilist statue, sumerian god, celestialpunk, beautiful, monochrome shiny, gloss\"]).cuda()\n",
    "        text = tokenizer([prompt]).cuda()\n",
    "        #text = tokenizer([f\"a 3d model of an astronaut riding a horse, 3d asset, high quality, not noisy, beautiful, black background\"]).cuda()\n",
    "        #text = tokenizer([\"a 3d model of the Mona Lisa, 3d asset, high quality, not noisy, beautiful, black background\"]).cuda()\n",
    "        #text = tokenizer([f\"a 3d model of a tabby cat, {view}, 3d asset, high quality, not noisy, beautiful, black background\"]).cuda()\n",
    "        text_vec = model.encode_text(text).float()\n",
    "        text_vec = text_vec / text_vec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        crops = random_crops(image.unsqueeze(0), 32, min_size=0.7, max_size=0.9)\n",
    "        img_vec = model.encode_image(clippp(crops).half()).float()\n",
    "        img_vec = img_vec / img_vec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        sim = img_vec @ text_vec.T\n",
    "        clip_loss = -sim.mean()\n",
    "        # self_sim = -(img_vec @ img_vec.T).mean()\n",
    "        # loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n",
    "        loss = clip_loss\n",
    "        loss.backward()\n",
    "\n",
    "        iter_end.record()\n",
    "\n",
    "        if ((iteration % 10) == 0 or iteration == opt.iterations) and iteration > 700:\n",
    "            rand_pos = torch.tensor([0.0, 0.0, camera_extent])\n",
    "            angle = 3.14159 * 2.0 * iteration / opt.iterations\n",
    "            rand_pos = camera_extent * torch.tensor([torch.tensor(angle).cos(), 0.0, torch.tensor(angle).sin()])\n",
    "\n",
    "            rot, trans = look_at(rand_pos, torch.tensor([0.0, 0.0, 0.0]), torch.tensor([0.0, 1.0, 0.0]))\n",
    "\n",
    "            dbg_cam = Camera(\n",
    "                colmap_id=0,\n",
    "                R=rot.numpy(), #rand_angle.numpy(),\n",
    "                T=trans.numpy(), #torch.zeros((3,)).numpy(),\n",
    "                FoVx=90.0,\n",
    "                FoVy=90.0,\n",
    "                image=torch.zeros((3, 512, 512)),\n",
    "                gt_alpha_mask=torch.ones((512, 512)),\n",
    "                image_name=\"test\",\n",
    "                uid=0,\n",
    "            )\n",
    "            image = render(dbg_cam, gaussians, pipe, background)[\"render\"]\n",
    "\n",
    "            show(image)\n",
    "            camera.snap()\n",
    "\n",
    "        if iteration == opt.iterations:\n",
    "            return camera.animate(blit = False, interval = 50), gaussians\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Progress bar\n",
    "            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n",
    "            if iteration % 10 == 0:\n",
    "                progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n",
    "                progress_bar.update(10)\n",
    "            if iteration == opt.iterations:\n",
    "                progress_bar.close()\n",
    "\n",
    "            # Log and save\n",
    "            # training_report(tb_writer, iteration, Ll1, loss, l1_loss, iter_start.elapsed_time(iter_end), testing_iterations, scene, render, (pipe, background))\n",
    "            if (iteration in saving_iterations):\n",
    "                print(\"\\n[ITER {}] Saving Gaussians\".format(iteration))\n",
    "                scene.save(iteration)\n",
    "\n",
    "            # Densification\n",
    "            if iteration < opt.densify_until_iter:\n",
    "                # Keep track of max radii in image-space for pruning\n",
    "                gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n",
    "\n",
    "                gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n",
    "\n",
    "                densify = iteration % opt.densification_interval == 0\n",
    "                if iteration > opt.densify_from_iter and densify:\n",
    "                    size_threshold = 20 if iteration > opt.opacity_reset_interval else None\n",
    "                    gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, camera_extent, size_threshold)\n",
    "\n",
    "                if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):\n",
    "                    gaussians.reset_opacity()\n",
    "\n",
    "            # Optimizer step\n",
    "            if iteration < opt.iterations and (iteration % 1) == 0:\n",
    "                gaussians.optimizer.step()\n",
    "                gaussians.optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            if (iteration in checkpoint_iterations):\n",
    "                print(\"\\n[ITER {}] Saving Checkpoint\".format(iteration))\n",
    "                torch.save((gaussians.capture(), iteration), scene.model_path + \"/chkpnt\" + str(iteration) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:10:47.974435Z",
     "iopub.status.busy": "2025-06-25T02:10:47.973179Z",
     "iopub.status.idle": "2025-06-25T02:10:47.988741Z",
     "shell.execute_reply": "2025-06-25T02:10:47.987243Z",
     "shell.execute_reply.started": "2025-06-25T02:10:47.974399Z"
    },
    "id": "TXFf4JYSwv-5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CvJjO-xxQg3"
   },
   "source": [
    "# Запуск обучения модели текстово-управляемого гауссовского сплаттинга\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом блоке происходит:\n",
    "\n",
    "- Задание текстового описания (промта) для генерации 3D-модели подсолнухов Ван Гога.\n",
    "- Инициализация параметров обучения и оптимизации через парсер аргументов.\n",
    "- Установка количества итераций и параметров скорости обучения.\n",
    "- Фиксация случайного сидa для воспроизводимости.\n",
    "- Запуск основной функции обучения `training` с заданными параметрами.\n",
    "- Отображение полученной анимации результата в формате HTML5-видео.\n",
    "\n",
    "Первый запуск может занять больше времени из-за компиляции (JIT) модели CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T03:19:04.657058Z",
     "iopub.status.busy": "2025-06-25T03:19:04.655898Z"
    },
    "id": "7A2BV9kPwyMU",
    "outputId": "ed34c22a-8ebb-40ec-9776-ff5274619bf8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scene.gaussian_model.GaussianModel object at 0x7f46517334f0>\n",
      "Number of points at initialisation :  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:   0%|          | 0/2500 [00:00<?, ?it/s]\u001b[A\n",
      "Training progress:   0%|          | 0/2500 [00:00<?, ?it/s, Loss=-0.1794784]\u001b[A\n",
      "Training progress:   0%|          | 10/2500 [00:00<02:39, 15.58it/s, Loss=-0.1794784]\u001b[A\n",
      "Training progress:   0%|          | 10/2500 [00:01<02:39, 15.58it/s, Loss=-0.2134091]\u001b[A\n",
      "Training progress:   1%|          | 20/2500 [00:01<02:24, 17.13it/s, Loss=-0.2134091]\u001b[A\n",
      "Training progress:   1%|          | 20/2500 [00:01<02:24, 17.13it/s, Loss=-0.2232087]\u001b[A\n",
      "Training progress:   1%|          | 30/2500 [00:01<02:19, 17.76it/s, Loss=-0.2232087]\u001b[A\n",
      "Training progress:   1%|          | 30/2500 [00:02<02:19, 17.76it/s, Loss=-0.2114120]\u001b[A\n",
      "Training progress:   2%|▏         | 40/2500 [00:02<02:15, 18.18it/s, Loss=-0.2114120]\u001b[A\n",
      "Training progress:   2%|▏         | 40/2500 [00:02<02:15, 18.18it/s, Loss=-0.2224677]\u001b[A\n",
      "Training progress:   2%|▏         | 50/2500 [00:02<02:13, 18.40it/s, Loss=-0.2224677]\u001b[A\n",
      "Training progress:   2%|▏         | 50/2500 [00:03<02:13, 18.40it/s, Loss=-0.2344400]\u001b[A\n",
      "Training progress:   2%|▏         | 60/2500 [00:03<02:26, 16.60it/s, Loss=-0.2344400]\u001b[A\n",
      "Training progress:   2%|▏         | 60/2500 [00:04<02:26, 16.60it/s, Loss=-0.2687100]\u001b[A\n",
      "Training progress:   3%|▎         | 70/2500 [00:04<02:34, 15.73it/s, Loss=-0.2687100]\u001b[A\n",
      "Training progress:   3%|▎         | 70/2500 [00:04<02:34, 15.73it/s, Loss=-0.2300216]\u001b[A\n",
      "Training progress:   3%|▎         | 80/2500 [00:04<02:34, 15.66it/s, Loss=-0.2300216]\u001b[A\n",
      "Training progress:   3%|▎         | 80/2500 [00:05<02:34, 15.66it/s, Loss=-0.2435541]\u001b[A\n",
      "Training progress:   4%|▎         | 90/2500 [00:05<02:25, 16.56it/s, Loss=-0.2435541]\u001b[A\n",
      "Training progress:   4%|▎         | 90/2500 [00:06<02:25, 16.56it/s, Loss=-0.2637201]\u001b[A\n",
      "Training progress:   4%|▍         | 100/2500 [00:06<02:29, 16.01it/s, Loss=-0.2637201]\u001b[A\n",
      "Training progress:   4%|▍         | 100/2500 [00:06<02:29, 16.01it/s, Loss=-0.2500754]\u001b[A\n",
      "Training progress:   4%|▍         | 110/2500 [00:06<02:23, 16.70it/s, Loss=-0.2500754]\u001b[A\n",
      "Training progress:   4%|▍         | 110/2500 [00:07<02:23, 16.70it/s, Loss=-0.2307011]\u001b[A\n",
      "Training progress:   5%|▍         | 120/2500 [00:07<02:26, 16.27it/s, Loss=-0.2307011]\u001b[A\n",
      "Training progress:   5%|▍         | 120/2500 [00:07<02:26, 16.27it/s, Loss=-0.2441503]\u001b[A\n",
      "Training progress:   5%|▌         | 130/2500 [00:07<02:28, 15.97it/s, Loss=-0.2441503]\u001b[A\n",
      "Training progress:   5%|▌         | 130/2500 [00:08<02:28, 15.97it/s, Loss=-0.2574357]\u001b[A\n",
      "Training progress:   6%|▌         | 140/2500 [00:08<02:31, 15.54it/s, Loss=-0.2574357]\u001b[A\n",
      "Training progress:   6%|▌         | 140/2500 [00:09<02:31, 15.54it/s, Loss=-0.2671852]\u001b[A\n",
      "Training progress:   6%|▌         | 150/2500 [00:09<02:30, 15.59it/s, Loss=-0.2671852]\u001b[A\n",
      "Training progress:   6%|▌         | 150/2500 [00:09<02:30, 15.59it/s, Loss=-0.2721541]\u001b[A\n",
      "Training progress:   6%|▋         | 160/2500 [00:09<02:27, 15.85it/s, Loss=-0.2721541]\u001b[A\n",
      "Training progress:   6%|▋         | 160/2500 [00:10<02:27, 15.85it/s, Loss=-0.2836269]\u001b[A\n",
      "Training progress:   7%|▋         | 170/2500 [00:10<02:29, 15.55it/s, Loss=-0.2836269]\u001b[A\n",
      "Training progress:   7%|▋         | 170/2500 [00:11<02:29, 15.55it/s, Loss=-0.2637863]\u001b[A\n",
      "Training progress:   7%|▋         | 180/2500 [00:11<02:29, 15.51it/s, Loss=-0.2637863]\u001b[A\n",
      "Training progress:   7%|▋         | 180/2500 [00:11<02:29, 15.51it/s, Loss=-0.2825279]\u001b[A\n",
      "Training progress:   8%|▊         | 190/2500 [00:11<02:20, 16.39it/s, Loss=-0.2825279]\u001b[A\n",
      "Training progress:   8%|▊         | 190/2500 [00:12<02:20, 16.39it/s, Loss=-0.2662026]\u001b[A\n",
      "Training progress:   8%|▊         | 200/2500 [00:12<02:16, 16.91it/s, Loss=-0.2662026]\u001b[A\n",
      "Training progress:   8%|▊         | 200/2500 [00:12<02:16, 16.91it/s, Loss=-0.2923373]\u001b[A\n",
      "Training progress:   8%|▊         | 210/2500 [00:12<02:12, 17.23it/s, Loss=-0.2923373]\u001b[A\n",
      "Training progress:   8%|▊         | 210/2500 [00:13<02:12, 17.23it/s, Loss=-0.2914854]\u001b[A\n",
      "Training progress:   9%|▉         | 220/2500 [00:13<02:18, 16.42it/s, Loss=-0.2914854]\u001b[A\n",
      "Training progress:   9%|▉         | 220/2500 [00:14<02:18, 16.42it/s, Loss=-0.2948907]\u001b[A\n",
      "Training progress:   9%|▉         | 230/2500 [00:14<02:25, 15.58it/s, Loss=-0.2948907]\u001b[A\n",
      "Training progress:   9%|▉         | 230/2500 [00:14<02:25, 15.58it/s, Loss=-0.2856089]\u001b[A\n",
      "Training progress:  10%|▉         | 240/2500 [00:14<02:27, 15.31it/s, Loss=-0.2856089]\u001b[A\n",
      "Training progress:  10%|▉         | 240/2500 [00:15<02:27, 15.31it/s, Loss=-0.3069190]\u001b[A\n",
      "Training progress:  10%|█         | 250/2500 [00:15<02:19, 16.16it/s, Loss=-0.3069190]\u001b[A\n",
      "Training progress:  10%|█         | 250/2500 [00:15<02:19, 16.16it/s, Loss=-0.3024640]\u001b[A\n",
      "Training progress:  10%|█         | 260/2500 [00:15<02:15, 16.57it/s, Loss=-0.3024640]\u001b[A\n",
      "Training progress:  10%|█         | 260/2500 [00:16<02:15, 16.57it/s, Loss=-0.3030834]\u001b[A\n",
      "Training progress:  11%|█         | 270/2500 [00:16<02:09, 17.25it/s, Loss=-0.3030834]\u001b[A\n",
      "Training progress:  11%|█         | 270/2500 [00:17<02:09, 17.25it/s, Loss=-0.3075836]\u001b[A\n",
      "Training progress:  11%|█         | 280/2500 [00:17<02:10, 16.98it/s, Loss=-0.3075836]\u001b[A\n",
      "Training progress:  11%|█         | 280/2500 [00:17<02:10, 16.98it/s, Loss=-0.3096764]\u001b[A\n",
      "Training progress:  12%|█▏        | 290/2500 [00:17<02:14, 16.42it/s, Loss=-0.3096764]\u001b[A\n",
      "Training progress:  12%|█▏        | 290/2500 [00:18<02:14, 16.42it/s, Loss=-0.3119279]\u001b[A\n",
      "Training progress:  12%|█▏        | 300/2500 [00:18<02:16, 16.15it/s, Loss=-0.3119279]\u001b[A\n",
      "Training progress:  12%|█▏        | 300/2500 [00:19<02:16, 16.15it/s, Loss=-0.3130682]\u001b[A\n",
      "Training progress:  12%|█▏        | 310/2500 [00:19<02:21, 15.52it/s, Loss=-0.3130682]\u001b[A\n",
      "Training progress:  12%|█▏        | 310/2500 [00:19<02:21, 15.52it/s, Loss=-0.3055729]\u001b[A\n",
      "Training progress:  13%|█▎        | 320/2500 [00:19<02:24, 15.13it/s, Loss=-0.3055729]\u001b[A\n",
      "Training progress:  13%|█▎        | 320/2500 [00:20<02:24, 15.13it/s, Loss=-0.3210954]\u001b[A\n",
      "Training progress:  13%|█▎        | 330/2500 [00:20<02:23, 15.13it/s, Loss=-0.3210954]\u001b[A\n",
      "Training progress:  13%|█▎        | 330/2500 [00:21<02:23, 15.13it/s, Loss=-0.3048703]\u001b[A\n",
      "Training progress:  14%|█▎        | 340/2500 [00:21<02:23, 15.00it/s, Loss=-0.3048703]\u001b[A\n",
      "Training progress:  14%|█▎        | 340/2500 [00:21<02:23, 15.00it/s, Loss=-0.3070675]\u001b[A\n",
      "Training progress:  14%|█▍        | 350/2500 [00:21<02:22, 15.08it/s, Loss=-0.3070675]\u001b[A\n",
      "Training progress:  14%|█▍        | 350/2500 [00:22<02:22, 15.08it/s, Loss=-0.3210873]\u001b[A\n",
      "Training progress:  14%|█▍        | 360/2500 [00:22<02:21, 15.11it/s, Loss=-0.3210873]\u001b[A\n",
      "Training progress:  14%|█▍        | 360/2500 [00:23<02:21, 15.11it/s, Loss=-0.3190285]\u001b[A\n",
      "Training progress:  15%|█▍        | 370/2500 [00:23<02:16, 15.64it/s, Loss=-0.3190285]\u001b[A\n",
      "Training progress:  15%|█▍        | 370/2500 [00:23<02:16, 15.64it/s, Loss=-0.3036925]\u001b[A\n",
      "Training progress:  15%|█▌        | 380/2500 [00:23<02:09, 16.31it/s, Loss=-0.3036925]\u001b[A\n",
      "Training progress:  15%|█▌        | 380/2500 [00:24<02:09, 16.31it/s, Loss=-0.3161531]\u001b[A\n",
      "Training progress:  16%|█▌        | 390/2500 [00:24<02:05, 16.83it/s, Loss=-0.3161531]\u001b[A\n",
      "Training progress:  16%|█▌        | 390/2500 [00:24<02:05, 16.83it/s, Loss=-0.3145970]\u001b[A\n",
      "Training progress:  16%|█▌        | 400/2500 [00:24<02:00, 17.44it/s, Loss=-0.3145970]\u001b[A\n",
      "Training progress:  16%|█▌        | 400/2500 [00:25<02:00, 17.44it/s, Loss=-0.3026132]\u001b[A\n",
      "Training progress:  16%|█▋        | 410/2500 [00:25<01:56, 17.88it/s, Loss=-0.3026132]\u001b[A\n",
      "Training progress:  16%|█▋        | 410/2500 [00:25<01:56, 17.88it/s, Loss=-0.3172052]\u001b[A\n",
      "Training progress:  17%|█▋        | 420/2500 [00:25<01:56, 17.79it/s, Loss=-0.3172052]\u001b[A\n",
      "Training progress:  17%|█▋        | 420/2500 [00:26<01:56, 17.79it/s, Loss=-0.3315995]\u001b[A\n",
      "Training progress:  17%|█▋        | 430/2500 [00:26<01:59, 17.33it/s, Loss=-0.3315995]\u001b[A\n",
      "Training progress:  17%|█▋        | 430/2500 [00:27<01:59, 17.33it/s, Loss=-0.3166965]\u001b[A\n",
      "Training progress:  18%|█▊        | 440/2500 [00:27<02:03, 16.65it/s, Loss=-0.3166965]\u001b[A\n",
      "Training progress:  18%|█▊        | 440/2500 [00:27<02:03, 16.65it/s, Loss=-0.3230043]\u001b[A\n",
      "Training progress:  18%|█▊        | 450/2500 [00:27<02:05, 16.36it/s, Loss=-0.3230043]\u001b[A\n",
      "Training progress:  18%|█▊        | 450/2500 [00:28<02:05, 16.36it/s, Loss=-0.3213346]\u001b[A\n",
      "Training progress:  18%|█▊        | 460/2500 [00:28<02:08, 15.83it/s, Loss=-0.3213346]\u001b[A\n",
      "Training progress:  18%|█▊        | 460/2500 [00:28<02:08, 15.83it/s, Loss=-0.3239811]\u001b[A\n",
      "Training progress:  19%|█▉        | 470/2500 [00:28<02:09, 15.65it/s, Loss=-0.3239811]\u001b[A\n",
      "Training progress:  19%|█▉        | 470/2500 [00:29<02:09, 15.65it/s, Loss=-0.3125842]\u001b[A\n",
      "Training progress:  19%|█▉        | 480/2500 [00:29<02:10, 15.44it/s, Loss=-0.3125842]\u001b[A\n",
      "Training progress:  19%|█▉        | 480/2500 [00:30<02:10, 15.44it/s, Loss=-0.3247283]\u001b[A\n",
      "Training progress:  20%|█▉        | 490/2500 [00:30<02:04, 16.16it/s, Loss=-0.3247283]\u001b[A\n",
      "Training progress:  20%|█▉        | 490/2500 [00:30<02:04, 16.16it/s, Loss=-0.3265536]\u001b[A\n",
      "Training progress:  20%|██        | 500/2500 [00:30<02:07, 15.70it/s, Loss=-0.3265536]\u001b[A\n",
      "Training progress:  20%|██        | 500/2500 [00:31<02:07, 15.70it/s, Loss=-0.3136902]\u001b[A\n",
      "Training progress:  20%|██        | 510/2500 [00:31<02:11, 15.17it/s, Loss=-0.3136902]\u001b[A\n",
      "Training progress:  20%|██        | 510/2500 [00:32<02:11, 15.17it/s, Loss=-0.3265533]\u001b[A\n",
      "Training progress:  21%|██        | 520/2500 [00:32<02:11, 15.09it/s, Loss=-0.3265533]\u001b[A\n",
      "Training progress:  21%|██        | 520/2500 [00:32<02:11, 15.09it/s, Loss=-0.3289025]\u001b[A\n",
      "Training progress:  21%|██        | 530/2500 [00:33<02:14, 14.65it/s, Loss=-0.3289025]\u001b[A\n",
      "Training progress:  21%|██        | 530/2500 [00:33<02:14, 14.65it/s, Loss=-0.3233482]\u001b[A\n",
      "Training progress:  22%|██▏       | 540/2500 [00:33<02:07, 15.37it/s, Loss=-0.3233482]\u001b[A\n",
      "Training progress:  22%|██▏       | 540/2500 [00:34<02:07, 15.37it/s, Loss=-0.3209810]\u001b[A\n",
      "Training progress:  22%|██▏       | 550/2500 [00:34<02:01, 16.11it/s, Loss=-0.3209810]\u001b[A\n",
      "Training progress:  22%|██▏       | 550/2500 [00:34<02:01, 16.11it/s, Loss=-0.3278761]\u001b[A\n",
      "Training progress:  22%|██▏       | 560/2500 [00:34<01:56, 16.68it/s, Loss=-0.3278761]\u001b[A\n",
      "Training progress:  22%|██▏       | 560/2500 [00:35<01:56, 16.68it/s, Loss=-0.3332151]\u001b[A\n",
      "Training progress:  23%|██▎       | 570/2500 [00:35<01:59, 16.10it/s, Loss=-0.3332151]\u001b[A\n",
      "Training progress:  23%|██▎       | 570/2500 [00:36<01:59, 16.10it/s, Loss=-0.3230148]\u001b[A\n",
      "Training progress:  23%|██▎       | 580/2500 [00:36<02:02, 15.67it/s, Loss=-0.3230148]\u001b[A\n",
      "Training progress:  23%|██▎       | 580/2500 [00:36<02:02, 15.67it/s, Loss=-0.3207209]\u001b[A\n",
      "Training progress:  24%|██▎       | 590/2500 [00:36<02:06, 15.13it/s, Loss=-0.3207209]\u001b[A\n",
      "Training progress:  24%|██▎       | 590/2500 [00:37<02:06, 15.13it/s, Loss=-0.3292479]\u001b[A\n",
      "Training progress:  24%|██▍       | 600/2500 [00:37<02:08, 14.81it/s, Loss=-0.3292479]\u001b[A\n",
      "Training progress:  24%|██▍       | 600/2500 [00:38<02:08, 14.81it/s, Loss=-0.3036446]\u001b[A\n",
      "Training progress:  24%|██▍       | 610/2500 [00:38<02:08, 14.68it/s, Loss=-0.3036446]\u001b[A\n",
      "Training progress:  24%|██▍       | 610/2500 [00:38<02:08, 14.68it/s, Loss=-0.3029216]\u001b[A\n",
      "Training progress:  25%|██▍       | 620/2500 [00:38<02:07, 14.79it/s, Loss=-0.3029216]\u001b[A\n",
      "Training progress:  25%|██▍       | 620/2500 [00:39<02:07, 14.79it/s, Loss=-0.3018747]\u001b[A\n",
      "Training progress:  25%|██▌       | 630/2500 [00:39<01:58, 15.75it/s, Loss=-0.3018747]\u001b[A\n",
      "Training progress:  25%|██▌       | 630/2500 [00:39<01:58, 15.75it/s, Loss=-0.3136228]\u001b[A\n",
      "Training progress:  26%|██▌       | 640/2500 [00:39<01:53, 16.42it/s, Loss=-0.3136228]\u001b[A\n",
      "Training progress:  26%|██▌       | 640/2500 [00:40<01:53, 16.42it/s, Loss=-0.3205485]\u001b[A\n",
      "Training progress:  26%|██▌       | 650/2500 [00:40<01:48, 17.03it/s, Loss=-0.3205485]\u001b[A\n",
      "Training progress:  26%|██▌       | 650/2500 [00:40<01:48, 17.03it/s, Loss=-0.3316813]\u001b[A\n",
      "Training progress:  26%|██▋       | 660/2500 [00:40<01:45, 17.38it/s, Loss=-0.3316813]\u001b[A\n",
      "Training progress:  26%|██▋       | 660/2500 [00:41<01:45, 17.38it/s, Loss=-0.3369415]\u001b[A\n",
      "Training progress:  27%|██▋       | 670/2500 [00:41<01:51, 16.47it/s, Loss=-0.3369415]\u001b[A\n",
      "Training progress:  27%|██▋       | 670/2500 [00:42<01:51, 16.47it/s, Loss=-0.3181345]\u001b[A\n",
      "Training progress:  27%|██▋       | 680/2500 [00:42<01:53, 16.02it/s, Loss=-0.3181345]\u001b[A\n",
      "Training progress:  27%|██▋       | 680/2500 [00:42<01:53, 16.02it/s, Loss=-0.3372302]\u001b[A\n",
      "Training progress:  28%|██▊       | 690/2500 [00:42<01:54, 15.85it/s, Loss=-0.3372302]\u001b[A\n",
      "Training progress:  28%|██▊       | 690/2500 [00:43<01:54, 15.85it/s, Loss=-0.3365658]\u001b[A\n",
      "Training progress:  28%|██▊       | 700/2500 [00:43<01:53, 15.80it/s, Loss=-0.3365658]\u001b[A\n",
      "Training progress:  28%|██▊       | 700/2500 [00:44<01:53, 15.80it/s, Loss=-0.2965612]\u001b[A\n",
      "Training progress:  28%|██▊       | 710/2500 [00:44<01:52, 15.87it/s, Loss=-0.2965612]\u001b[A\n",
      "Training progress:  28%|██▊       | 710/2500 [00:44<01:52, 15.87it/s, Loss=-0.3366121]\u001b[A\n",
      "Training progress:  29%|██▉       | 720/2500 [00:44<01:47, 16.58it/s, Loss=-0.3366121]\u001b[A\n",
      "Training progress:  29%|██▉       | 720/2500 [00:45<01:47, 16.58it/s, Loss=-0.3307658]\u001b[A\n",
      "Training progress:  29%|██▉       | 730/2500 [00:45<01:43, 17.07it/s, Loss=-0.3307658]\u001b[A\n",
      "Training progress:  29%|██▉       | 730/2500 [00:45<01:43, 17.07it/s, Loss=-0.3363613]\u001b[A\n",
      "Training progress:  30%|██▉       | 740/2500 [00:45<01:43, 17.00it/s, Loss=-0.3363613]\u001b[A\n",
      "Training progress:  30%|██▉       | 740/2500 [00:46<01:43, 17.00it/s, Loss=-0.3356561]\u001b[A\n",
      "Training progress:  30%|███       | 750/2500 [00:46<01:41, 17.20it/s, Loss=-0.3356561]\u001b[A\n",
      "Training progress:  30%|███       | 750/2500 [00:47<01:41, 17.20it/s, Loss=-0.3250778]\u001b[A\n",
      "Training progress:  30%|███       | 760/2500 [00:47<01:40, 17.26it/s, Loss=-0.3250778]\u001b[A\n",
      "Training progress:  30%|███       | 760/2500 [00:47<01:40, 17.26it/s, Loss=-0.3520253]\u001b[A\n",
      "Training progress:  31%|███       | 770/2500 [00:47<01:39, 17.36it/s, Loss=-0.3520253]\u001b[A\n",
      "Training progress:  31%|███       | 770/2500 [00:48<01:39, 17.36it/s, Loss=-0.3552309]\u001b[A\n",
      "Training progress:  31%|███       | 780/2500 [00:48<01:44, 16.39it/s, Loss=-0.3552309]\u001b[A\n",
      "Training progress:  31%|███       | 780/2500 [00:48<01:44, 16.39it/s, Loss=-0.3464306]\u001b[A\n",
      "Training progress:  32%|███▏      | 790/2500 [00:48<01:47, 15.97it/s, Loss=-0.3464306]\u001b[A\n",
      "Training progress:  32%|███▏      | 790/2500 [00:49<01:47, 15.97it/s, Loss=-0.3504629]\u001b[A\n",
      "Training progress:  32%|███▏      | 800/2500 [00:49<01:43, 16.41it/s, Loss=-0.3504629]\u001b[A\n",
      "Training progress:  32%|███▏      | 800/2500 [00:50<01:43, 16.41it/s, Loss=-0.3264733]\u001b[A\n",
      "Training progress:  32%|███▏      | 810/2500 [00:50<01:41, 16.59it/s, Loss=-0.3264733]\u001b[A\n",
      "Training progress:  32%|███▏      | 810/2500 [00:50<01:41, 16.59it/s, Loss=-0.3236022]\u001b[A\n",
      "Training progress:  33%|███▎      | 820/2500 [00:50<01:39, 16.82it/s, Loss=-0.3236022]\u001b[A\n",
      "Training progress:  33%|███▎      | 820/2500 [00:51<01:39, 16.82it/s, Loss=-0.3276543]\u001b[A\n",
      "Training progress:  33%|███▎      | 830/2500 [00:51<01:38, 17.04it/s, Loss=-0.3276543]\u001b[A\n",
      "Training progress:  33%|███▎      | 830/2500 [00:51<01:38, 17.04it/s, Loss=-0.3457928]\u001b[A\n",
      "Training progress:  34%|███▎      | 840/2500 [00:51<01:39, 16.70it/s, Loss=-0.3457928]\u001b[A\n",
      "Training progress:  34%|███▎      | 840/2500 [00:52<01:39, 16.70it/s, Loss=-0.3437678]\u001b[A\n",
      "Training progress:  34%|███▍      | 850/2500 [00:52<01:45, 15.60it/s, Loss=-0.3437678]\u001b[A\n",
      "Training progress:  34%|███▍      | 850/2500 [00:53<01:45, 15.60it/s, Loss=-0.3648846]\u001b[A\n",
      "Training progress:  34%|███▍      | 860/2500 [00:53<01:48, 15.15it/s, Loss=-0.3648846]\u001b[A\n",
      "Training progress:  34%|███▍      | 860/2500 [00:54<01:48, 15.15it/s, Loss=-0.3665595]\u001b[A\n",
      "Training progress:  35%|███▍      | 870/2500 [00:54<01:48, 14.99it/s, Loss=-0.3665595]\u001b[A\n",
      "Training progress:  35%|███▍      | 870/2500 [00:54<01:48, 14.99it/s, Loss=-0.3593855]\u001b[A\n",
      "Training progress:  35%|███▌      | 880/2500 [00:54<01:52, 14.43it/s, Loss=-0.3593855]\u001b[A\n",
      "Training progress:  35%|███▌      | 880/2500 [00:55<01:52, 14.43it/s, Loss=-0.3702222]\u001b[A\n",
      "Training progress:  36%|███▌      | 890/2500 [00:55<01:49, 14.65it/s, Loss=-0.3702222]\u001b[A\n",
      "Training progress:  36%|███▌      | 890/2500 [00:56<01:49, 14.65it/s, Loss=-0.3799899]\u001b[A\n",
      "Training progress:  36%|███▌      | 900/2500 [00:56<01:50, 14.42it/s, Loss=-0.3799899]\u001b[A\n",
      "Training progress:  36%|███▌      | 900/2500 [00:56<01:50, 14.42it/s, Loss=-0.3172066]\u001b[A\n",
      "Training progress:  36%|███▋      | 910/2500 [00:56<01:53, 13.97it/s, Loss=-0.3172066]\u001b[A\n",
      "Training progress:  36%|███▋      | 910/2500 [00:57<01:53, 13.97it/s, Loss=-0.3324722]\u001b[A\n",
      "Training progress:  37%|███▋      | 920/2500 [00:57<01:54, 13.83it/s, Loss=-0.3324722]\u001b[A\n",
      "Training progress:  37%|███▋      | 920/2500 [00:58<01:54, 13.83it/s, Loss=-0.3452858]\u001b[A\n",
      "Training progress:  37%|███▋      | 930/2500 [00:58<01:54, 13.68it/s, Loss=-0.3452858]\u001b[A\n",
      "Training progress:  37%|███▋      | 930/2500 [00:59<01:54, 13.68it/s, Loss=-0.3478971]\u001b[A\n",
      "Training progress:  38%|███▊      | 940/2500 [00:59<01:53, 13.77it/s, Loss=-0.3478971]\u001b[A\n",
      "Training progress:  38%|███▊      | 940/2500 [00:59<01:53, 13.77it/s, Loss=-0.3547858]\u001b[A\n",
      "Training progress:  38%|███▊      | 950/2500 [00:59<01:52, 13.81it/s, Loss=-0.3547858]\u001b[A\n",
      "Training progress:  38%|███▊      | 950/2500 [01:00<01:52, 13.81it/s, Loss=-0.3526442]\u001b[A\n",
      "Training progress:  38%|███▊      | 960/2500 [01:00<01:50, 13.93it/s, Loss=-0.3526442]\u001b[A\n",
      "Training progress:  38%|███▊      | 960/2500 [01:01<01:50, 13.93it/s, Loss=-0.3538571]\u001b[A\n",
      "Training progress:  39%|███▉      | 970/2500 [01:01<01:43, 14.76it/s, Loss=-0.3538571]\u001b[A\n",
      "Training progress:  39%|███▉      | 970/2500 [01:01<01:43, 14.76it/s, Loss=-0.3566344]\u001b[A\n",
      "Training progress:  39%|███▉      | 980/2500 [01:01<01:39, 15.32it/s, Loss=-0.3566344]\u001b[A\n",
      "Training progress:  39%|███▉      | 980/2500 [01:02<01:39, 15.32it/s, Loss=-0.3657863]\u001b[A\n",
      "Training progress:  40%|███▉      | 990/2500 [01:02<01:35, 15.77it/s, Loss=-0.3657863]\u001b[A\n",
      "Training progress:  40%|███▉      | 990/2500 [01:02<01:35, 15.77it/s, Loss=-0.3757937]\u001b[A\n",
      "Training progress:  40%|████      | 1000/2500 [01:02<01:33, 16.13it/s, Loss=-0.3757937]\u001b[A\n",
      "Training progress:  40%|████      | 1000/2500 [01:03<01:33, 16.13it/s, Loss=-0.3469699]\u001b[A\n",
      "Training progress:  40%|████      | 1010/2500 [01:03<01:33, 15.98it/s, Loss=-0.3469699]\u001b[A\n",
      "Training progress:  40%|████      | 1010/2500 [01:04<01:33, 15.98it/s, Loss=-0.3673803]\u001b[A\n",
      "Training progress:  41%|████      | 1020/2500 [01:04<01:30, 16.36it/s, Loss=-0.3673803]\u001b[A\n",
      "Training progress:  41%|████      | 1020/2500 [01:04<01:30, 16.36it/s, Loss=-0.3618049]\u001b[A\n",
      "Training progress:  41%|████      | 1030/2500 [01:04<01:27, 16.71it/s, Loss=-0.3618049]\u001b[A\n",
      "Training progress:  41%|████      | 1030/2500 [01:05<01:27, 16.71it/s, Loss=-0.3604895]\u001b[A\n",
      "Training progress:  42%|████▏     | 1040/2500 [01:05<01:32, 15.86it/s, Loss=-0.3604895]\u001b[A\n",
      "Training progress:  42%|████▏     | 1040/2500 [01:06<01:32, 15.86it/s, Loss=-0.3821681]\u001b[A\n",
      "Training progress:  42%|████▏     | 1050/2500 [01:06<01:31, 15.84it/s, Loss=-0.3821681]\u001b[A\n",
      "Training progress:  42%|████▏     | 1050/2500 [01:06<01:31, 15.84it/s, Loss=-0.3624628]\u001b[A\n",
      "Training progress:  42%|████▏     | 1060/2500 [01:06<01:28, 16.21it/s, Loss=-0.3624628]\u001b[A\n",
      "Training progress:  42%|████▏     | 1060/2500 [01:07<01:28, 16.21it/s, Loss=-0.3795534]\u001b[A\n",
      "Training progress:  43%|████▎     | 1070/2500 [01:07<01:27, 16.37it/s, Loss=-0.3795534]\u001b[A\n",
      "Training progress:  43%|████▎     | 1070/2500 [01:07<01:27, 16.37it/s, Loss=-0.3695715]\u001b[A\n",
      "Training progress:  43%|████▎     | 1080/2500 [01:07<01:25, 16.63it/s, Loss=-0.3695715]\u001b[A\n",
      "Training progress:  43%|████▎     | 1080/2500 [01:08<01:25, 16.63it/s, Loss=-0.3810067]\u001b[A\n",
      "Training progress:  44%|████▎     | 1090/2500 [01:08<01:25, 16.44it/s, Loss=-0.3810067]\u001b[A\n",
      "Training progress:  44%|████▎     | 1090/2500 [01:09<01:25, 16.44it/s, Loss=-0.3769031]\u001b[A\n",
      "Training progress:  44%|████▍     | 1100/2500 [01:09<01:24, 16.55it/s, Loss=-0.3769031]\u001b[A\n",
      "Training progress:  44%|████▍     | 1100/2500 [01:09<01:24, 16.55it/s, Loss=-0.3363139]\u001b[A\n",
      "Training progress:  44%|████▍     | 1110/2500 [01:09<01:28, 15.65it/s, Loss=-0.3363139]\u001b[A\n",
      "Training progress:  44%|████▍     | 1110/2500 [01:10<01:28, 15.65it/s, Loss=-0.3756157]\u001b[A\n",
      "Training progress:  45%|████▍     | 1120/2500 [01:10<01:31, 15.16it/s, Loss=-0.3756157]\u001b[A\n",
      "Training progress:  45%|████▍     | 1120/2500 [01:11<01:31, 15.16it/s, Loss=-0.3683083]\u001b[A\n",
      "Training progress:  45%|████▌     | 1130/2500 [01:11<01:26, 15.76it/s, Loss=-0.3683083]\u001b[A\n",
      "Training progress:  45%|████▌     | 1130/2500 [01:11<01:26, 15.76it/s, Loss=-0.3761083]\u001b[A\n",
      "Training progress:  46%|████▌     | 1140/2500 [01:11<01:24, 16.18it/s, Loss=-0.3761083]\u001b[A\n",
      "Training progress:  46%|████▌     | 1140/2500 [01:12<01:24, 16.18it/s, Loss=-0.3758099]\u001b[A\n",
      "Training progress:  46%|████▌     | 1150/2500 [01:12<01:22, 16.36it/s, Loss=-0.3758099]\u001b[A\n",
      "Training progress:  46%|████▌     | 1150/2500 [01:12<01:22, 16.36it/s, Loss=-0.3679078]\u001b[A\n",
      "Training progress:  46%|████▋     | 1160/2500 [01:12<01:20, 16.60it/s, Loss=-0.3679078]\u001b[A\n",
      "Training progress:  46%|████▋     | 1160/2500 [01:13<01:20, 16.60it/s, Loss=-0.3917673]\u001b[A\n",
      "Training progress:  47%|████▋     | 1170/2500 [01:13<01:20, 16.58it/s, Loss=-0.3917673]\u001b[A\n",
      "Training progress:  47%|████▋     | 1170/2500 [01:13<01:20, 16.58it/s, Loss=-0.3915202]\u001b[A\n",
      "Training progress:  47%|████▋     | 1180/2500 [01:13<01:18, 16.72it/s, Loss=-0.3915202]\u001b[A\n",
      "Training progress:  47%|████▋     | 1180/2500 [01:14<01:18, 16.72it/s, Loss=-0.3957716]\u001b[A\n",
      "Training progress:  48%|████▊     | 1190/2500 [01:14<01:17, 16.93it/s, Loss=-0.3957716]\u001b[A\n",
      "Training progress:  48%|████▊     | 1190/2500 [01:15<01:17, 16.93it/s, Loss=-0.3733457]\u001b[A\n",
      "Training progress:  48%|████▊     | 1200/2500 [01:15<01:20, 16.09it/s, Loss=-0.3733457]\u001b[A\n",
      "Training progress:  48%|████▊     | 1200/2500 [01:15<01:20, 16.09it/s, Loss=-0.3642731]\u001b[A\n",
      "Training progress:  48%|████▊     | 1210/2500 [01:15<01:23, 15.38it/s, Loss=-0.3642731]\u001b[A\n",
      "Training progress:  48%|████▊     | 1210/2500 [01:16<01:23, 15.38it/s, Loss=-0.3727304]\u001b[A\n",
      "Training progress:  49%|████▉     | 1220/2500 [01:16<01:27, 14.69it/s, Loss=-0.3727304]\u001b[A\n",
      "Training progress:  49%|████▉     | 1220/2500 [01:17<01:27, 14.69it/s, Loss=-0.3719006]\u001b[A\n",
      "Training progress:  49%|████▉     | 1230/2500 [01:17<01:29, 14.25it/s, Loss=-0.3719006]\u001b[A\n",
      "Training progress:  49%|████▉     | 1230/2500 [01:18<01:29, 14.25it/s, Loss=-0.3716821]\u001b[A\n",
      "Training progress:  50%|████▉     | 1240/2500 [01:18<01:28, 14.16it/s, Loss=-0.3716821]\u001b[A\n",
      "Training progress:  50%|████▉     | 1240/2500 [01:18<01:28, 14.16it/s, Loss=-0.3827337]\u001b[A\n",
      "Training progress:  50%|█████     | 1250/2500 [01:18<01:24, 14.82it/s, Loss=-0.3827337]\u001b[A\n",
      "Training progress:  50%|█████     | 1250/2500 [01:19<01:24, 14.82it/s, Loss=-0.3858040]\u001b[A\n",
      "Training progress:  50%|█████     | 1260/2500 [01:19<01:25, 14.43it/s, Loss=-0.3858040]\u001b[A\n",
      "Training progress:  50%|█████     | 1260/2500 [01:20<01:25, 14.43it/s, Loss=-0.4089610]\u001b[A\n",
      "Training progress:  51%|█████     | 1270/2500 [01:20<01:22, 14.88it/s, Loss=-0.4089610]\u001b[A\n",
      "Training progress:  51%|█████     | 1270/2500 [01:20<01:22, 14.88it/s, Loss=-0.3995783]\u001b[A\n",
      "Training progress:  51%|█████     | 1280/2500 [01:20<01:19, 15.37it/s, Loss=-0.3995783]\u001b[A\n",
      "Training progress:  51%|█████     | 1280/2500 [01:21<01:19, 15.37it/s, Loss=-0.3992873]\u001b[A\n",
      "Training progress:  52%|█████▏    | 1290/2500 [01:21<01:17, 15.70it/s, Loss=-0.3992873]\u001b[A\n",
      "Training progress:  52%|█████▏    | 1290/2500 [01:21<01:17, 15.70it/s, Loss=-0.4064672]\u001b[A\n",
      "Training progress:  52%|█████▏    | 1300/2500 [01:21<01:14, 16.03it/s, Loss=-0.4064672]\u001b[A\n",
      "Training progress:  52%|█████▏    | 1300/2500 [01:22<01:14, 16.03it/s, Loss=-0.3805773]\u001b[A\n",
      "Training progress:  52%|█████▏    | 1310/2500 [01:22<01:14, 15.96it/s, Loss=-0.3805773]\u001b[A\n",
      "Training progress:  52%|█████▏    | 1310/2500 [01:23<01:14, 15.96it/s, Loss=-0.3779972]\u001b[A\n",
      "Training progress:  53%|█████▎    | 1320/2500 [01:23<01:13, 15.96it/s, Loss=-0.3779972]\u001b[A\n",
      "Training progress:  53%|█████▎    | 1320/2500 [01:23<01:13, 15.96it/s, Loss=-0.3951954]\u001b[A\n",
      "Training progress:  53%|█████▎    | 1330/2500 [01:23<01:13, 16.02it/s, Loss=-0.3951954]\u001b[A\n",
      "Training progress:  53%|█████▎    | 1330/2500 [01:24<01:13, 16.02it/s, Loss=-0.3902013]\u001b[A\n",
      "Training progress:  54%|█████▎    | 1340/2500 [01:24<01:13, 15.87it/s, Loss=-0.3902013]\u001b[A\n",
      "Training progress:  54%|█████▎    | 1340/2500 [01:25<01:13, 15.87it/s, Loss=-0.4000895]\u001b[A\n",
      "Training progress:  54%|█████▍    | 1350/2500 [01:25<01:12, 15.89it/s, Loss=-0.4000895]\u001b[A\n",
      "Training progress:  54%|█████▍    | 1350/2500 [01:25<01:12, 15.89it/s, Loss=-0.4031331]\u001b[A\n",
      "Training progress:  54%|█████▍    | 1360/2500 [01:25<01:11, 15.85it/s, Loss=-0.4031331]\u001b[A\n",
      "Training progress:  54%|█████▍    | 1360/2500 [01:26<01:11, 15.85it/s, Loss=-0.4080646]\u001b[A\n",
      "Training progress:  55%|█████▍    | 1370/2500 [01:26<01:13, 15.41it/s, Loss=-0.4080646]\u001b[A\n",
      "Training progress:  55%|█████▍    | 1370/2500 [01:27<01:13, 15.41it/s, Loss=-0.4009576]\u001b[A\n",
      "Training progress:  55%|█████▌    | 1380/2500 [01:27<01:16, 14.60it/s, Loss=-0.4009576]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  55%|█████▌    | 1380/2500 [01:27<01:16, 14.60it/s, Loss=-0.4106680]\u001b[A\n",
      "Training progress:  56%|█████▌    | 1390/2500 [01:27<01:17, 14.25it/s, Loss=-0.4106680]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  56%|█████▌    | 1390/2500 [01:28<01:17, 14.25it/s, Loss=-0.4105310]\u001b[A\n",
      "Training progress:  56%|█████▌    | 1400/2500 [01:28<01:21, 13.52it/s, Loss=-0.4105310]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  56%|█████▌    | 1400/2500 [01:29<01:21, 13.52it/s, Loss=-0.3933875]\u001b[A\n",
      "Training progress:  56%|█████▋    | 1410/2500 [01:29<01:23, 13.03it/s, Loss=-0.3933875]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  56%|█████▋    | 1410/2500 [01:30<01:23, 13.03it/s, Loss=-0.3982099]\u001b[A\n",
      "Training progress:  57%|█████▋    | 1420/2500 [01:30<01:22, 13.04it/s, Loss=-0.3982099]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  57%|█████▋    | 1420/2500 [01:31<01:22, 13.04it/s, Loss=-0.4040406]\u001b[A\n",
      "Training progress:  57%|█████▋    | 1430/2500 [01:31<01:23, 12.77it/s, Loss=-0.4040406]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  57%|█████▋    | 1430/2500 [01:31<01:23, 12.77it/s, Loss=-0.4255610]\u001b[A\n",
      "Training progress:  58%|█████▊    | 1440/2500 [01:31<01:19, 13.34it/s, Loss=-0.4255610]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  58%|█████▊    | 1440/2500 [01:32<01:19, 13.34it/s, Loss=-0.4214613]\u001b[A\n",
      "Training progress:  58%|█████▊    | 1450/2500 [01:32<01:19, 13.19it/s, Loss=-0.4214613]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  58%|█████▊    | 1450/2500 [01:33<01:19, 13.19it/s, Loss=-0.4291130]\u001b[A\n",
      "Training progress:  58%|█████▊    | 1460/2500 [01:33<01:18, 13.19it/s, Loss=-0.4291130]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  58%|█████▊    | 1460/2500 [01:34<01:18, 13.19it/s, Loss=-0.4197502]\u001b[A\n",
      "Training progress:  59%|█████▉    | 1470/2500 [01:34<01:14, 13.76it/s, Loss=-0.4197502]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  59%|█████▉    | 1470/2500 [01:34<01:14, 13.76it/s, Loss=-0.4307237]\u001b[A\n",
      "Training progress:  59%|█████▉    | 1480/2500 [01:34<01:15, 13.55it/s, Loss=-0.4307237]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  59%|█████▉    | 1480/2500 [01:35<01:15, 13.55it/s, Loss=-0.4319494]\u001b[A\n",
      "Training progress:  60%|█████▉    | 1490/2500 [01:35<01:12, 13.96it/s, Loss=-0.4319494]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  60%|█████▉    | 1490/2500 [01:36<01:12, 13.96it/s, Loss=-0.4292957]\u001b[A\n",
      "Training progress:  60%|██████    | 1500/2500 [01:36<01:10, 14.23it/s, Loss=-0.4292957]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  60%|██████    | 1500/2500 [01:36<01:10, 14.23it/s, Loss=-0.4122103]\u001b[A\n",
      "Training progress:  60%|██████    | 1510/2500 [01:36<01:09, 14.26it/s, Loss=-0.4122103]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  60%|██████    | 1510/2500 [01:37<01:09, 14.26it/s, Loss=-0.4018617]\u001b[A\n",
      "Training progress:  61%|██████    | 1520/2500 [01:37<01:09, 14.18it/s, Loss=-0.4018617]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  61%|██████    | 1520/2500 [01:38<01:09, 14.18it/s, Loss=-0.4137781]\u001b[A\n",
      "Training progress:  61%|██████    | 1530/2500 [01:38<01:07, 14.27it/s, Loss=-0.4137781]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  61%|██████    | 1530/2500 [01:38<01:07, 14.27it/s, Loss=-0.4246983]\u001b[A\n",
      "Training progress:  62%|██████▏   | 1540/2500 [01:38<01:07, 14.31it/s, Loss=-0.4246983]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  62%|██████▏   | 1540/2500 [01:39<01:07, 14.31it/s, Loss=-0.4344902]\u001b[A\n",
      "Training progress:  62%|██████▏   | 1550/2500 [01:39<01:06, 14.32it/s, Loss=-0.4344902]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  62%|██████▏   | 1550/2500 [01:40<01:06, 14.32it/s, Loss=-0.4442764]\u001b[A\n",
      "Training progress:  62%|██████▏   | 1560/2500 [01:40<01:10, 13.40it/s, Loss=-0.4442764]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  62%|██████▏   | 1560/2500 [01:41<01:10, 13.40it/s, Loss=-0.4508719]\u001b[A\n",
      "Training progress:  63%|██████▎   | 1570/2500 [01:41<01:11, 13.04it/s, Loss=-0.4508719]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  63%|██████▎   | 1570/2500 [01:41<01:11, 13.04it/s, Loss=-0.4313475]\u001b[A\n",
      "Training progress:  63%|██████▎   | 1580/2500 [01:41<01:08, 13.38it/s, Loss=-0.4313475]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  63%|██████▎   | 1580/2500 [01:42<01:08, 13.38it/s, Loss=-0.4390274]\u001b[A\n",
      "Training progress:  64%|██████▎   | 1590/2500 [01:42<01:06, 13.66it/s, Loss=-0.4390274]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  64%|██████▎   | 1590/2500 [01:43<01:06, 13.66it/s, Loss=-0.4564544]\u001b[A\n",
      "Training progress:  64%|██████▍   | 1600/2500 [01:43<01:04, 13.86it/s, Loss=-0.4564544]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  64%|██████▍   | 1600/2500 [01:44<01:04, 13.86it/s, Loss=-0.4254286]\u001b[A\n",
      "Training progress:  64%|██████▍   | 1610/2500 [01:44<01:11, 12.43it/s, Loss=-0.4254286]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  64%|██████▍   | 1610/2500 [01:45<01:11, 12.43it/s, Loss=-0.4398746]\u001b[A\n",
      "Training progress:  65%|██████▍   | 1620/2500 [01:45<01:10, 12.52it/s, Loss=-0.4398746]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  65%|██████▍   | 1620/2500 [01:45<01:10, 12.52it/s, Loss=-0.4468901]\u001b[A\n",
      "Training progress:  65%|██████▌   | 1630/2500 [01:45<01:08, 12.78it/s, Loss=-0.4468901]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  65%|██████▌   | 1630/2500 [01:46<01:08, 12.78it/s, Loss=-0.4452195]\u001b[A\n",
      "Training progress:  66%|██████▌   | 1640/2500 [01:46<01:10, 12.26it/s, Loss=-0.4452195]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  66%|██████▌   | 1640/2500 [01:47<01:10, 12.26it/s, Loss=-0.4428573]\u001b[A\n",
      "Training progress:  66%|██████▌   | 1650/2500 [01:47<01:07, 12.51it/s, Loss=-0.4428573]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  66%|██████▌   | 1650/2500 [01:48<01:07, 12.51it/s, Loss=-0.4552183]\u001b[A\n",
      "Training progress:  66%|██████▋   | 1660/2500 [01:48<01:05, 12.77it/s, Loss=-0.4552183]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  66%|██████▋   | 1660/2500 [01:49<01:05, 12.77it/s, Loss=-0.4531325]\u001b[A\n",
      "Training progress:  67%|██████▋   | 1670/2500 [01:49<01:04, 12.92it/s, Loss=-0.4531325]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  67%|██████▋   | 1670/2500 [01:49<01:04, 12.92it/s, Loss=-0.4599977]\u001b[A\n",
      "Training progress:  67%|██████▋   | 1680/2500 [01:49<01:05, 12.58it/s, Loss=-0.4599977]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  67%|██████▋   | 1680/2500 [01:50<01:05, 12.58it/s, Loss=-0.4589241]\u001b[A\n",
      "Training progress:  68%|██████▊   | 1690/2500 [01:50<01:03, 12.81it/s, Loss=-0.4589241]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  68%|██████▊   | 1690/2500 [01:51<01:03, 12.81it/s, Loss=-0.4635965]\u001b[A\n",
      "Training progress:  68%|██████▊   | 1700/2500 [01:51<01:02, 12.75it/s, Loss=-0.4635965]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  68%|██████▊   | 1700/2500 [01:52<01:02, 12.75it/s, Loss=-0.4358510]\u001b[A\n",
      "Training progress:  68%|██████▊   | 1710/2500 [01:52<01:03, 12.51it/s, Loss=-0.4358510]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  68%|██████▊   | 1710/2500 [01:53<01:03, 12.51it/s, Loss=-0.4547440]\u001b[A\n",
      "Training progress:  69%|██████▉   | 1720/2500 [01:53<01:05, 11.98it/s, Loss=-0.4547440]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  69%|██████▉   | 1720/2500 [01:54<01:05, 11.98it/s, Loss=-0.4483406]\u001b[A\n",
      "Training progress:  69%|██████▉   | 1730/2500 [01:54<01:04, 12.02it/s, Loss=-0.4483406]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  69%|██████▉   | 1730/2500 [01:54<01:04, 12.02it/s, Loss=-0.4436864]\u001b[A\n",
      "Training progress:  70%|██████▉   | 1740/2500 [01:54<01:03, 11.91it/s, Loss=-0.4436864]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  70%|██████▉   | 1740/2500 [01:55<01:03, 11.91it/s, Loss=-0.4605115]\u001b[A\n",
      "Training progress:  70%|███████   | 1750/2500 [01:55<01:03, 11.90it/s, Loss=-0.4605115]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  70%|███████   | 1750/2500 [01:56<01:03, 11.90it/s, Loss=-0.4576490]\u001b[A\n",
      "Training progress:  70%|███████   | 1760/2500 [01:56<01:03, 11.70it/s, Loss=-0.4576490]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  70%|███████   | 1760/2500 [01:57<01:03, 11.70it/s, Loss=-0.4628243]\u001b[A\n",
      "Training progress:  71%|███████   | 1770/2500 [01:57<01:01, 11.80it/s, Loss=-0.4628243]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  71%|███████   | 1770/2500 [01:58<01:01, 11.80it/s, Loss=-0.4708190]\u001b[A\n",
      "Training progress:  71%|███████   | 1780/2500 [01:58<01:00, 11.97it/s, Loss=-0.4708190]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  71%|███████   | 1780/2500 [01:59<01:00, 11.97it/s, Loss=-0.4707877]\u001b[A\n",
      "Training progress:  72%|███████▏  | 1790/2500 [01:59<01:00, 11.71it/s, Loss=-0.4707877]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  72%|███████▏  | 1790/2500 [01:59<01:00, 11.71it/s, Loss=-0.4811535]\u001b[A\n",
      "Training progress:  72%|███████▏  | 1800/2500 [01:59<00:58, 11.90it/s, Loss=-0.4811535]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  72%|███████▏  | 1800/2500 [02:01<00:58, 11.90it/s, Loss=-0.4568262]\u001b[A\n",
      "Training progress:  72%|███████▏  | 1810/2500 [02:01<01:10,  9.78it/s, Loss=-0.4568262]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  72%|███████▏  | 1810/2500 [02:03<01:10,  9.78it/s, Loss=-0.4616051]\u001b[A\n",
      "Training progress:  73%|███████▎  | 1820/2500 [02:03<01:23,  8.15it/s, Loss=-0.4616051]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  73%|███████▎  | 1820/2500 [02:04<01:23,  8.15it/s, Loss=-0.4612278]\u001b[A\n",
      "Training progress:  73%|███████▎  | 1830/2500 [02:04<01:29,  7.46it/s, Loss=-0.4612278]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  73%|███████▎  | 1830/2500 [02:06<01:29,  7.46it/s, Loss=-0.4553953]\u001b[A\n",
      "Training progress:  74%|███████▎  | 1840/2500 [02:06<01:32,  7.15it/s, Loss=-0.4553953]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  74%|███████▎  | 1840/2500 [02:07<01:32,  7.15it/s, Loss=-0.4695728]\u001b[A\n",
      "Training progress:  74%|███████▍  | 1850/2500 [02:07<01:35,  6.81it/s, Loss=-0.4695728]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  74%|███████▍  | 1850/2500 [02:09<01:35,  6.81it/s, Loss=-0.4734343]\u001b[A\n",
      "Training progress:  74%|███████▍  | 1860/2500 [02:09<01:35,  6.71it/s, Loss=-0.4734343]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  74%|███████▍  | 1860/2500 [02:11<01:35,  6.71it/s, Loss=-0.4693216]\u001b[A\n",
      "Training progress:  75%|███████▍  | 1870/2500 [02:11<01:36,  6.50it/s, Loss=-0.4693216]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  75%|███████▍  | 1870/2500 [02:12<01:36,  6.50it/s, Loss=-0.4719047]\u001b[A\n",
      "Training progress:  75%|███████▌  | 1880/2500 [02:12<01:36,  6.45it/s, Loss=-0.4719047]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  75%|███████▌  | 1880/2500 [02:14<01:36,  6.45it/s, Loss=-0.4657035]\u001b[A\n",
      "Training progress:  76%|███████▌  | 1890/2500 [02:14<01:35,  6.36it/s, Loss=-0.4657035]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  76%|███████▌  | 1890/2500 [02:15<01:35,  6.36it/s, Loss=-0.4807544]\u001b[A\n",
      "Training progress:  76%|███████▌  | 1900/2500 [02:15<01:35,  6.30it/s, Loss=-0.4807544]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  76%|███████▌  | 1900/2500 [02:17<01:35,  6.30it/s, Loss=-0.4571293]\u001b[A\n",
      "Training progress:  76%|███████▋  | 1910/2500 [02:17<01:38,  5.96it/s, Loss=-0.4571293]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  76%|███████▋  | 1910/2500 [02:19<01:38,  5.96it/s, Loss=-0.4583669]\u001b[A\n",
      "Training progress:  77%|███████▋  | 1920/2500 [02:19<01:40,  5.80it/s, Loss=-0.4583669]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  77%|███████▋  | 1920/2500 [02:21<01:40,  5.80it/s, Loss=-0.4560708]\u001b[A\n",
      "Training progress:  77%|███████▋  | 1930/2500 [02:21<01:40,  5.70it/s, Loss=-0.4560708]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  77%|███████▋  | 1930/2500 [02:23<01:40,  5.70it/s, Loss=-0.4716861]\u001b[A\n",
      "Training progress:  78%|███████▊  | 1940/2500 [02:23<01:40,  5.59it/s, Loss=-0.4716861]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  78%|███████▊  | 1940/2500 [02:25<01:40,  5.59it/s, Loss=-0.4698184]\u001b[A\n",
      "Training progress:  78%|███████▊  | 1950/2500 [02:25<01:39,  5.55it/s, Loss=-0.4698184]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  78%|███████▊  | 1950/2500 [02:26<01:39,  5.55it/s, Loss=-0.4822718]\u001b[A\n",
      "Training progress:  78%|███████▊  | 1960/2500 [02:26<01:37,  5.51it/s, Loss=-0.4822718]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  78%|███████▊  | 1960/2500 [02:28<01:37,  5.51it/s, Loss=-0.4841157]\u001b[A\n",
      "Training progress:  79%|███████▉  | 1970/2500 [02:28<01:36,  5.47it/s, Loss=-0.4841157]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  79%|███████▉  | 1970/2500 [02:30<01:36,  5.47it/s, Loss=-0.4853714]\u001b[A\n",
      "Training progress:  79%|███████▉  | 1980/2500 [02:30<01:36,  5.40it/s, Loss=-0.4853714]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  79%|███████▉  | 1980/2500 [02:32<01:36,  5.40it/s, Loss=-0.4898522]\u001b[A\n",
      "Training progress:  80%|███████▉  | 1990/2500 [02:32<01:35,  5.35it/s, Loss=-0.4898522]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  80%|███████▉  | 1990/2500 [02:34<01:35,  5.35it/s, Loss=-0.4796460]\u001b[A\n",
      "Training progress:  80%|████████  | 2000/2500 [02:34<01:32,  5.39it/s, Loss=-0.4796460]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  80%|████████  | 2000/2500 [02:36<01:32,  5.39it/s, Loss=-0.4725208]\u001b[A\n",
      "Training progress:  80%|████████  | 2010/2500 [02:36<01:36,  5.10it/s, Loss=-0.4725208]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  80%|████████  | 2010/2500 [02:38<01:36,  5.10it/s, Loss=-0.4741505]\u001b[A\n",
      "Training progress:  81%|████████  | 2020/2500 [02:38<01:38,  4.86it/s, Loss=-0.4741505]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  81%|████████  | 2020/2500 [02:41<01:38,  4.86it/s, Loss=-0.4803961]\u001b[A\n",
      "Training progress:  81%|████████  | 2030/2500 [02:41<01:41,  4.63it/s, Loss=-0.4803961]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  81%|████████  | 2030/2500 [02:43<01:41,  4.63it/s, Loss=-0.4650339]\u001b[A\n",
      "Training progress:  82%|████████▏ | 2040/2500 [02:43<01:42,  4.49it/s, Loss=-0.4650339]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  82%|████████▏ | 2040/2500 [02:46<01:42,  4.49it/s, Loss=-0.4851180]\u001b[A\n",
      "Training progress:  82%|████████▏ | 2050/2500 [02:46<01:42,  4.39it/s, Loss=-0.4851180]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  82%|████████▏ | 2050/2500 [02:48<01:42,  4.39it/s, Loss=-0.4752698]\u001b[A\n",
      "Training progress:  82%|████████▏ | 2060/2500 [02:48<01:41,  4.32it/s, Loss=-0.4752698]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  82%|████████▏ | 2060/2500 [02:50<01:41,  4.32it/s, Loss=-0.4894094]\u001b[A\n",
      "Training progress:  83%|████████▎ | 2070/2500 [02:50<01:40,  4.29it/s, Loss=-0.4894094]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  83%|████████▎ | 2070/2500 [02:53<01:40,  4.29it/s, Loss=-0.4902121]\u001b[A\n",
      "Training progress:  83%|████████▎ | 2080/2500 [02:53<01:37,  4.29it/s, Loss=-0.4902121]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  83%|████████▎ | 2080/2500 [02:55<01:37,  4.29it/s, Loss=-0.4880636]\u001b[A\n",
      "Training progress:  84%|████████▎ | 2090/2500 [02:55<01:35,  4.28it/s, Loss=-0.4880636]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  84%|████████▎ | 2090/2500 [02:57<01:35,  4.28it/s, Loss=-0.4883049]\u001b[A\n",
      "Training progress:  84%|████████▍ | 2100/2500 [02:57<01:33,  4.26it/s, Loss=-0.4883049]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  84%|████████▍ | 2100/2500 [03:00<01:33,  4.26it/s, Loss=-0.4747978]\u001b[A\n",
      "Training progress:  84%|████████▍ | 2110/2500 [03:00<01:33,  4.16it/s, Loss=-0.4747978]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  84%|████████▍ | 2110/2500 [03:03<01:33,  4.16it/s, Loss=-0.4769994]\u001b[A\n",
      "Training progress:  85%|████████▍ | 2120/2500 [03:03<01:35,  3.98it/s, Loss=-0.4769994]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  85%|████████▍ | 2120/2500 [03:06<01:35,  3.98it/s, Loss=-0.4836763]\u001b[A\n",
      "Training progress:  85%|████████▌ | 2130/2500 [03:06<01:35,  3.88it/s, Loss=-0.4836763]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  85%|████████▌ | 2130/2500 [03:08<01:35,  3.88it/s, Loss=-0.4838788]\u001b[A\n",
      "Training progress:  86%|████████▌ | 2140/2500 [03:08<01:33,  3.83it/s, Loss=-0.4838788]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  86%|████████▌ | 2140/2500 [03:11<01:33,  3.83it/s, Loss=-0.4920974]\u001b[A\n",
      "Training progress:  86%|████████▌ | 2150/2500 [03:11<01:31,  3.84it/s, Loss=-0.4920974]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  86%|████████▌ | 2150/2500 [03:13<01:31,  3.84it/s, Loss=-0.4866848]\u001b[A\n",
      "Training progress:  86%|████████▋ | 2160/2500 [03:13<01:28,  3.83it/s, Loss=-0.4866848]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  86%|████████▋ | 2160/2500 [03:16<01:28,  3.83it/s, Loss=-0.4723907]\u001b[A\n",
      "Training progress:  87%|████████▋ | 2170/2500 [03:16<01:27,  3.78it/s, Loss=-0.4723907]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  87%|████████▋ | 2170/2500 [03:19<01:27,  3.78it/s, Loss=-0.4906139]\u001b[A\n",
      "Training progress:  87%|████████▋ | 2180/2500 [03:19<01:25,  3.75it/s, Loss=-0.4906139]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  87%|████████▋ | 2180/2500 [03:21<01:25,  3.75it/s, Loss=-0.4927775]\u001b[A\n",
      "Training progress:  88%|████████▊ | 2190/2500 [03:21<01:22,  3.78it/s, Loss=-0.4927775]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  88%|████████▊ | 2190/2500 [03:24<01:22,  3.78it/s, Loss=-0.5014742]\u001b[A\n",
      "Training progress:  88%|████████▊ | 2200/2500 [03:24<01:20,  3.74it/s, Loss=-0.5014742]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  88%|████████▊ | 2200/2500 [03:28<01:20,  3.74it/s, Loss=-0.4781238]\u001b[A\n",
      "Training progress:  88%|████████▊ | 2210/2500 [03:28<01:24,  3.41it/s, Loss=-0.4781238]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  88%|████████▊ | 2210/2500 [03:31<01:24,  3.41it/s, Loss=-0.4830312]\u001b[A\n",
      "Training progress:  89%|████████▉ | 2220/2500 [03:31<01:27,  3.21it/s, Loss=-0.4830312]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  89%|████████▉ | 2220/2500 [03:35<01:27,  3.21it/s, Loss=-0.4820993]\u001b[A\n",
      "Training progress:  89%|████████▉ | 2230/2500 [03:35<01:27,  3.08it/s, Loss=-0.4820993]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  89%|████████▉ | 2230/2500 [03:38<01:27,  3.08it/s, Loss=-0.4942563]\u001b[A\n",
      "Training progress:  90%|████████▉ | 2240/2500 [03:38<01:26,  3.01it/s, Loss=-0.4942563]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  90%|████████▉ | 2240/2500 [03:42<01:26,  3.01it/s, Loss=-0.4942863]\u001b[A\n",
      "Training progress:  90%|█████████ | 2250/2500 [03:42<01:24,  2.96it/s, Loss=-0.4942863]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  90%|█████████ | 2250/2500 [03:45<01:24,  2.96it/s, Loss=-0.4862987]\u001b[A\n",
      "Training progress:  90%|█████████ | 2260/2500 [03:45<01:22,  2.92it/s, Loss=-0.4862987]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  90%|█████████ | 2260/2500 [03:49<01:22,  2.92it/s, Loss=-0.4906197]\u001b[A\n",
      "Training progress:  91%|█████████ | 2270/2500 [03:49<01:19,  2.89it/s, Loss=-0.4906197]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  91%|█████████ | 2270/2500 [03:52<01:19,  2.89it/s, Loss=-0.5011465]\u001b[A\n",
      "Training progress:  91%|█████████ | 2280/2500 [03:52<01:16,  2.87it/s, Loss=-0.5011465]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  91%|█████████ | 2280/2500 [03:56<01:16,  2.87it/s, Loss=-0.4925624]\u001b[A\n",
      "Training progress:  92%|█████████▏| 2290/2500 [03:56<01:13,  2.87it/s, Loss=-0.4925624]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  92%|█████████▏| 2290/2500 [03:59<01:13,  2.87it/s, Loss=-0.4931253]\u001b[A\n",
      "Training progress:  92%|█████████▏| 2300/2500 [03:59<01:10,  2.85it/s, Loss=-0.4931253]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  92%|█████████▏| 2300/2500 [04:04<01:10,  2.85it/s, Loss=-0.4877581]\u001b[A\n",
      "Training progress:  92%|█████████▏| 2310/2500 [04:04<01:09,  2.72it/s, Loss=-0.4877581]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  92%|█████████▏| 2310/2500 [04:08<01:09,  2.72it/s, Loss=-0.4947926]\u001b[A\n",
      "Training progress:  93%|█████████▎| 2320/2500 [04:08<01:08,  2.63it/s, Loss=-0.4947926]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  93%|█████████▎| 2320/2500 [04:12<01:08,  2.63it/s, Loss=-0.4860187]\u001b[A\n",
      "Training progress:  93%|█████████▎| 2330/2500 [04:12<01:06,  2.57it/s, Loss=-0.4860187]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  93%|█████████▎| 2330/2500 [04:16<01:06,  2.57it/s, Loss=-0.4922603]\u001b[A\n",
      "Training progress:  94%|█████████▎| 2340/2500 [04:16<01:02,  2.54it/s, Loss=-0.4922603]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  94%|█████████▎| 2340/2500 [04:20<01:02,  2.54it/s, Loss=-0.4812557]\u001b[A\n",
      "Training progress:  94%|█████████▍| 2350/2500 [04:20<00:59,  2.51it/s, Loss=-0.4812557]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  94%|█████████▍| 2350/2500 [04:24<00:59,  2.51it/s, Loss=-0.4926152]\u001b[A\n",
      "Training progress:  94%|█████████▍| 2360/2500 [04:24<00:55,  2.50it/s, Loss=-0.4926152]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  94%|█████████▍| 2360/2500 [04:28<00:55,  2.50it/s, Loss=-0.4893001]\u001b[A\n",
      "Training progress:  95%|█████████▍| 2370/2500 [04:28<00:52,  2.48it/s, Loss=-0.4893001]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  95%|█████████▍| 2370/2500 [04:32<00:52,  2.48it/s, Loss=-0.4973305]\u001b[A\n",
      "Training progress:  95%|█████████▌| 2380/2500 [04:32<00:48,  2.47it/s, Loss=-0.4973305]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  95%|█████████▌| 2380/2500 [04:36<00:48,  2.47it/s, Loss=-0.5007084]\u001b[A\n",
      "Training progress:  96%|█████████▌| 2390/2500 [04:36<00:44,  2.46it/s, Loss=-0.5007084]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  96%|█████████▌| 2390/2500 [04:40<00:44,  2.46it/s, Loss=-0.5021849]\u001b[A\n",
      "Training progress:  96%|█████████▌| 2400/2500 [04:40<00:40,  2.46it/s, Loss=-0.5021849]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  96%|█████████▌| 2400/2500 [04:45<00:40,  2.46it/s, Loss=-0.4856735]\u001b[A\n",
      "Training progress:  96%|█████████▋| 2410/2500 [04:45<00:38,  2.34it/s, Loss=-0.4856735]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  96%|█████████▋| 2410/2500 [04:50<00:38,  2.34it/s, Loss=-0.4736918]\u001b[A\n",
      "Training progress:  97%|█████████▋| 2420/2500 [04:50<00:35,  2.28it/s, Loss=-0.4736918]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  97%|█████████▋| 2420/2500 [04:54<00:35,  2.28it/s, Loss=-0.5055151]\u001b[A\n",
      "Training progress:  97%|█████████▋| 2430/2500 [04:54<00:31,  2.23it/s, Loss=-0.5055151]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  97%|█████████▋| 2430/2500 [04:59<00:31,  2.23it/s, Loss=-0.4872661]\u001b[A\n",
      "Training progress:  98%|█████████▊| 2440/2500 [04:59<00:27,  2.21it/s, Loss=-0.4872661]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  98%|█████████▊| 2440/2500 [05:04<00:27,  2.21it/s, Loss=-0.5002215]\u001b[A\n",
      "Training progress:  98%|█████████▊| 2450/2500 [05:04<00:22,  2.19it/s, Loss=-0.5002215]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  98%|█████████▊| 2450/2500 [05:08<00:22,  2.19it/s, Loss=-0.4948143]\u001b[A\n",
      "Training progress:  98%|█████████▊| 2460/2500 [05:08<00:18,  2.18it/s, Loss=-0.4948143]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  98%|█████████▊| 2460/2500 [05:13<00:18,  2.18it/s, Loss=-0.5003614]\u001b[A\n",
      "Training progress:  99%|█████████▉| 2470/2500 [05:13<00:13,  2.18it/s, Loss=-0.5003614]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  99%|█████████▉| 2470/2500 [05:18<00:13,  2.18it/s, Loss=-0.5031790]\u001b[A\n",
      "Training progress:  99%|█████████▉| 2480/2500 [05:18<00:09,  2.17it/s, Loss=-0.5031790]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "\n",
      "Training progress:  99%|█████████▉| 2480/2500 [05:22<00:09,  2.17it/s, Loss=-0.5079869]\u001b[A\n",
      "Training progress: 100%|█████████▉| 2490/2500 [05:22<00:04,  2.16it/s, Loss=-0.5079869]\u001b[AWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training progress: 100%|█████████▉| 2490/2500 [05:27<00:01,  7.61it/s, Loss=-0.5079869]\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "prompt = \"a 3d model of Van Gogh's Sunflowers, 3d asset, high quality, not noisy, beautiful, black background\"\n",
    "#prompt = \"a 3d model of a tabby cat, {}, 3d asset, high quality, not noisy, beautiful, black background\"\n",
    "# Put a {} in your prompt to have view depedent prompting\n",
    "# i.e \"a cat, {}\" -> \"a cat, front view\", \"a cat, side view\" etc\n",
    "\n",
    "parser = ArgumentParser()\n",
    "model_params = ModelParams(parser)\n",
    "opt_params = OptimizationParams(parser)\n",
    "pp = PipelineParams(parser)\n",
    "args, _ = parser.parse_known_args()\n",
    "args.iterations = 2500\n",
    "args.position_lr_init = 1e-2\n",
    "args.position_lr_final = 1e-5\n",
    "args.position_lr_max_steps = args.iterations\n",
    "# args.opacity_reset_interval = 5000\n",
    "torch.manual_seed(2023)\n",
    "animation, gaussians = training(prompt, model_params.extract(args), opt_params.extract(args), pp.extract(args), [], [], [], None, -1)\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:59:46.038755Z",
     "iopub.status.busy": "2025-06-25T01:59:46.037139Z",
     "iopub.status.idle": "2025-06-25T02:00:09.013545Z",
     "shell.execute_reply": "2025-06-25T02:00:09.012081Z",
     "shell.execute_reply.started": "2025-06-25T01:59:46.038723Z"
    },
    "id": "Ann-4cndwy5B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gaussians.save_ply(\"../outputTC.ply\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4b67fe9364194dde98fba540ae3d3391": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a47289814d1044419e4912e6b3e19c42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf626f8d3fd048b4a7b74c69d8fab2d4",
       "IPY_MODEL_bb898f4ccc2f4958b8c086d2ac3142a5",
       "IPY_MODEL_db1fb2dddc38406cb92a8a22ac410828"
      ],
      "layout": "IPY_MODEL_f008a1a83a524a04b15ba1468e9911d6"
     }
    },
    "b8c5a59ed4f1441c95e46bd2b70aad18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb898f4ccc2f4958b8c086d2ac3142a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b67fe9364194dde98fba540ae3d3391",
      "max": 605219813,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d594d6782180447881fa68b8e39f13e2",
      "value": 605219813
     }
    },
    "ceb88275c4044807a17ecafca5ce8d72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf626f8d3fd048b4a7b74c69d8fab2d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0812836e6734413b4679cfe0a8c6844",
      "placeholder": "​",
      "style": "IPY_MODEL_b8c5a59ed4f1441c95e46bd2b70aad18",
      "value": "open_clip_pytorch_model.bin: 100%"
     }
    },
    "d594d6782180447881fa68b8e39f13e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db1fb2dddc38406cb92a8a22ac410828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceb88275c4044807a17ecafca5ce8d72",
      "placeholder": "​",
      "style": "IPY_MODEL_faead77034ea40a99f7344a7e2b50934",
      "value": " 605M/605M [00:06&lt;00:00, 112MB/s]"
     }
    },
    "e0812836e6734413b4679cfe0a8c6844": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f008a1a83a524a04b15ba1468e9911d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faead77034ea40a99f7344a7e2b50934": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
